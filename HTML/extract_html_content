import requests
import json
import time
import os
from bs4 import BeautifulSoup
from urllib.parse import urlparse

def load_hiring_threads():
    """Load the hiring threads from the JSON file"""
    try:
        with open('JSON analysis/URLs/all_hiring_threads.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print("Error: all_hiring_threads.json not found. Please run the scraper first.")
        return []

def save_html_content(url, content, month, year):
    """Save HTML content to a file"""
    # Create directory if it doesn't exist
    output_dir = os.path.join('JSON analysis', 'HTML')
    os.makedirs(output_dir, exist_ok=True)
    
    # Create filename based on month and year
    filename = f"hiring_thread_{month}_{year}.html"
    filepath = os.path.join(output_dir, filename)
    
    # Save the content
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"Saved HTML content to {filepath}")
    return filepath

def get_html_content(url):
    """Get HTML content from a URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching {url}: {str(e)}")
        return None

def get_item(item_id):
    """Get an item from the Hacker News API"""
    url = f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json"
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        if data is None:  # API returns null for deleted/missing items
            return None
        return data
    except Exception as e:
        print(f"Error fetching item {item_id}: {str(e)}")
        return None

def get_comments_recursive(comment_id, depth=0, max_depth=3):
    """Recursively get comments and their replies"""
    if depth > max_depth:
        return None
    
    comment = get_item(comment_id)
    if not comment or comment.get('deleted') or comment.get('dead'):
        return None
    
    result = {
        'id': comment.get('id'),
        'text': comment.get('text', ''),
        'by': comment.get('by', ''),
        'time': comment.get('time', 0),
        'replies': []
    }
    
    # Get child comments if any
    if 'kids' in comment:
        for kid_id in comment['kids']:
            child_comment = get_comments_recursive(kid_id, depth + 1, max_depth)
            if child_comment:
                result['replies'].append(child_comment)
            time.sleep(0.1)  # Be polite to the API
    
    return result

def get_thread_comments(story_id):
    """Get all comments from a story"""
    try:
        story_id = int(story_id)  # Convert to integer
    except ValueError:
        print(f"Invalid story ID: {story_id}")
        return []
    
    story = get_item(story_id)
    if not story:
        print(f"Could not fetch story {story_id}")
        return []
    
    if 'kids' not in story:
        print(f"No comments found in story {story_id}")
        return []
    
    comments = []
    total_comments = len(story['kids'])
    print(f"Found {total_comments} top-level comments in story {story_id}")
    
    # For testing, just get first 5 comments
    for i, comment_id in enumerate(story['kids'][:5], 1):
        print(f"Fetching comment {i}/5")
        comment_tree = get_comments_recursive(comment_id)
        if comment_tree:
            comments.append(comment_tree)
        time.sleep(0.1)  # Be polite to the API
    
    return comments

def extract_story_id(url):
    """Extract the story ID from a Hacker News URL"""
    try:
        return url.split('id=')[1].split('&')[0]  # Handle any additional URL parameters
    except:
        return None

def save_script_to_path():
    """Save the current script to the JSON analysis/HTML directory"""
    script_path = os.path.join('JSON analysis', 'HTML', 'extract_html_content.py')
    with open(__file__, 'r', encoding='utf-8') as source:
        with open(script_path, 'w', encoding='utf-8') as dest:
            dest.write(source.read())
    print(f"Saved script to {script_path}")

def combine_html_to_json():
    """Combine all HTML files into one JSON file"""
    html_dir = os.path.join('JSON analysis', 'HTML')
    combined_data = []
    
    for filename in os.listdir(html_dir):
        if filename.endswith('.html') and filename.startswith('hiring_thread_'):
            filepath = os.path.join(html_dir, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                # Extract month and year from filename
                parts = filename.replace('hiring_thread_', '').replace('.html', '').split('_')
                month = parts[0]
                year = parts[1]
                combined_data.append({
                    'month': month,
                    'year': year,
                    'filename': filename,
                    'content': content
                })
    
    # Save combined data to JSON
    output_file = os.path.join(html_dir, 'all_threads_combined.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(combined_data, f, indent=2)
    print(f"Combined all HTML files into {output_file}")

def main():
    # Save the script first
    save_script_to_path()
    
    threads = load_hiring_threads()
    if not threads:
        return
    
    print(f"Processing {len(threads)} threads...\n")
    
    for i, thread in enumerate(threads, 1):
        print(f"\nProcessing thread {i}/{len(threads)}: {thread['title']}")
        
        # Get HTML content
        html_content = get_html_content(thread['url'])
        if html_content:
            # Save the content
            save_html_content(thread['url'], html_content, thread['month'], thread['year'])
        
        # Be polite to the server
        time.sleep(1)
    
    # Combine all HTML files into one JSON
    combine_html_to_json()
    
    print("\nProcessing complete! All HTML content saved to JSON analysis/HTML/")

if __name__ == "__main__":
    main() 
